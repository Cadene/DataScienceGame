{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming raw DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/youtube\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphlab as gl\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gl.SFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: Expected 16 fields in line 23582, saw 17\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e357ddf4d988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Importing raw data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpd_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/data/train_sample.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#pd_test = pd.DataFrame.from_csv(folder + '/data/test_sample.csv',sep=';',header=0,index_col=None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m   1036\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    472\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m _parser_defaults = {\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    719\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:7544)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:7784)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:8401)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas/parser.c:8275)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:20691)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCParserError\u001b[0m: Error tokenizing data. C error: Expected 16 fields in line 23582, saw 17\n"
     ]
    }
   ],
   "source": [
    "#Importing raw data\n",
    "pd_train = pd.DataFrame.from_csv(folder + '/data/train_sample.csv',sep=',',header=0,index_col=None, encoding='utf-8')\n",
    "#pd_test = pd.DataFrame.from_csv(folder + '/data/test_sample.csv',sep=';',header=0,index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding some new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RemovePonctuationAndStemming(r):\n",
    "    new_r = tokenizer.tokenize(r)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    out = [stemmer.stem(word) for word in new_r]    \n",
    "    return out\n",
    "\n",
    "\n",
    "def addStuff(pd):\n",
    "    pd['lenght'] = pd['Sentence'].apply(lambda r: len(r)).astype(float)\n",
    "    \n",
    "    #number of CAPS character\n",
    "    pd['nb_caps_char'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter.isupper()])).astype(float)\n",
    "    \n",
    "    #number of CAPS word\n",
    "    pd['nb_caps_word'] = pd['Sentence'].apply(lambda r: np.sum([1 for word in r.split(\" \") if len(word) > 2 if word.isupper()])).astype(float)\n",
    "    \n",
    "    #one column for each punctuation token\n",
    "    pd['nb_?'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='?'])  )\n",
    "    \n",
    "    pd['nb_,'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==','])  )\n",
    "    \n",
    "    pd['nb_!'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='!'])  )\n",
    "    \n",
    "    pd['nb_.'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='.'])  )\n",
    "    \n",
    "    pd['nb_;'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==';'])  )\n",
    "    \n",
    "    pd[\"nb_'\"] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==\"'\"])  )\n",
    "    \n",
    "    pd['nb_all'] = pd['nb_?']+pd['nb_,']+pd['nb_!']+pd['nb_.']+pd['nb_;']+pd[\"nb_'\"]\n",
    "    \n",
    "    pd['nb_,;'] = pd['nb_,'] * pd['nb_;']\n",
    "    \n",
    "    #diversiy\n",
    "    pd['diversity'] = pd['Sentence'].apply(lambda r: float(len(np.unique(r.split(\" \")))) / float(len(r)))    \n",
    "    \n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train = addStuff(pd_train)\n",
    "pd_test = addStuff(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's onkores, Bilgewater?\",\n",
       "       \"He wouldn't ever dared to talk such talk in his life before.\",\n",
       "       \"Then she got to talking about her husband, and about her relations up the river, and her relations down the river, and about how much better off they used to was, and how they didn't know but they'd made a mistake coming to our town, instead of letting well aloneand so on and so on, till I was afeard I had made a mistake coming to her to find out what was going on in the town; but by and by she dropped on to pap and the murder, and then I was pretty willing to let her clatter right along.\",\n",
       "       ..., 'HAMLET Marry, this is miching mallecho; it means mischief.',\n",
       "       \"LORD POLONIUS At such a time I'll loose my daughter to him: Be you and I behind an arras then; Mark the encounter: if he love her not And be not from his reason fall'n thereon, Let me be no assistant for a state, But keep a farm and carters.\",\n",
       "       \"MARCELLUS Let's do't, I pray; and I this morning know Where we shall find him most conveniently.\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train['Sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[-,.?!;]\")\n",
    "def TagStringwithPunct(r):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(r)\n",
    "    tagged_list = pos_tag(tokenized)\n",
    "    \n",
    "    final_str = []\n",
    "    for word,tag in tagged_list:\n",
    "        final_str.append(tag+\"_\"+word)\n",
    "    \n",
    "    \n",
    "    return ' '.join(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving munged DATA into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train.to_pickle(folder +'/data_munged/pd_train_munged')\n",
    "pd_test.to_pickle(folder +'/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
