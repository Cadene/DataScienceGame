{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming raw DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/youtube\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv(folder + '/data/train_sample.csv', sep=',', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)\n",
    "\n",
    "\n",
    "pd_test = pd.read_csv(folder + '/data/test_sample.csv', sep=',', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'video_category_id', u'title', u'description', u'published_at',\n",
      "       u'viewCount', u'likeCount', u'dislikeCount', u'favoriteCount',\n",
      "       u'commentCount', u'duration', u'dimension', u'definition', u'caption',\n",
      "       u'licensedContent', u'topicIds', u'relevantTopicIds'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print pd_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split topicIds\n",
    "pd_train[u'topicIds'] = pd_train[u'topicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "pd_test[u'topicIds'] = pd_test[u'topicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "\n",
    "#split\n",
    "pd_train[u'relevantTopicIds'] = pd_train[u'relevantTopicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "pd_test[u'relevantTopicIds'] = pd_test[u'relevantTopicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_category_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>dislikeCount</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>dimension</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>licensedContent</th>\n",
       "      <th>topicIds</th>\n",
       "      <th>relevantTopicIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>19</td>\n",
       "      <td>臺灣觀光六大主題「讚！臺灣」生態篇(3分鐘版)；\"Bravo! Taiwan\"-NATURE...</td>\n",
       "      <td>2014-05-20T09:34:11.000Z</td>\n",
       "      <td>27559</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>PT3M1S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>/m/0zwm09x</td>\n",
       "      <td>[/m/06f32, /m/011s0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_category_id                                              title  \\\n",
       "16646                 19  臺灣觀光六大主題「讚！臺灣」生態篇(3分鐘版)；\"Bravo! Taiwan\"-NATURE...   \n",
       "\n",
       "                    description published_at  viewCount  likeCount  \\\n",
       "16646  2014-05-20T09:34:11.000Z        27559        356          1   \n",
       "\n",
       "       dislikeCount  favoriteCount commentCount duration dimension definition  \\\n",
       "16646             0             12       PT3M1S       2d        hd      false   \n",
       "\n",
       "      caption licensedContent              topicIds relevantTopicIds  \n",
       "16646   False      /m/0zwm09x  [/m/06f32, /m/011s0]               []  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train[pd_train['published_at'].apply(lambda r: len(r)!=24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2014, 5, 20)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date(2014,5,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "year is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-a2b2c9a20757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdateutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m24\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2058\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2060\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2061\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:58435)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-117-a2b2c9a20757>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(r)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdateutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m24\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mrepl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweekday\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: year is out of range"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "pd_train['published_at'].apply(lambda r: parser.parse(r) if len(r)!=24 else datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "year is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-bbdd91dead9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2058\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2060\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2061\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:58435)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-bbdd91dead9f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(r)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mrepl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweekday\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: year is out of range"
     ]
    }
   ],
   "source": [
    "pd_train['published_at'].apply(lambda r: parser.parse(r) if )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-11-15T17:00:06.000Z\n"
     ]
    }
   ],
   "source": [
    "test = pd_train['published_at'][0] ; print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2014\n"
     ]
    }
   ],
   "source": [
    "print d.strftime('%m/%d/%Y') #==> '09/26/2008'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding some new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RemovePonctuationAndStemming(r):\n",
    "    new_r = tokenizer.tokenize(r)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    out = [stemmer.stem(word) for word in new_r]    \n",
    "    return out\n",
    "\n",
    "\n",
    "def addStuff(pd):\n",
    "    pd['lenght'] = pd['Sentence'].apply(lambda r: len(r)).astype(float)\n",
    "    \n",
    "    #number of CAPS character\n",
    "    pd['nb_caps_char'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter.isupper()])).astype(float)\n",
    "    \n",
    "    #number of CAPS word\n",
    "    pd['nb_caps_word'] = pd['Sentence'].apply(lambda r: np.sum([1 for word in r.split(\" \") if len(word) > 2 if word.isupper()])).astype(float)\n",
    "    \n",
    "    #one column for each punctuation token\n",
    "    pd['nb_?'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='?'])  )\n",
    "    \n",
    "    pd['nb_,'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==','])  )\n",
    "    \n",
    "    pd['nb_!'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='!'])  )\n",
    "    \n",
    "    pd['nb_.'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='.'])  )\n",
    "    \n",
    "    pd['nb_;'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==';'])  )\n",
    "    \n",
    "    pd[\"nb_'\"] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==\"'\"])  )\n",
    "    \n",
    "    pd['nb_all'] = pd['nb_?']+pd['nb_,']+pd['nb_!']+pd['nb_.']+pd['nb_;']+pd[\"nb_'\"]\n",
    "    \n",
    "    pd['nb_,;'] = pd['nb_,'] * pd['nb_;']\n",
    "    \n",
    "    #diversiy\n",
    "    pd['diversity'] = pd['Sentence'].apply(lambda r: float(len(np.unique(r.split(\" \")))) / float(len(r)))    \n",
    "    \n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train = addStuff(pd_train)\n",
    "pd_test = addStuff(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's onkores, Bilgewater?\",\n",
       "       \"He wouldn't ever dared to talk such talk in his life before.\",\n",
       "       \"Then she got to talking about her husband, and about her relations up the river, and her relations down the river, and about how much better off they used to was, and how they didn't know but they'd made a mistake coming to our town, instead of letting well aloneand so on and so on, till I was afeard I had made a mistake coming to her to find out what was going on in the town; but by and by she dropped on to pap and the murder, and then I was pretty willing to let her clatter right along.\",\n",
       "       ..., 'HAMLET Marry, this is miching mallecho; it means mischief.',\n",
       "       \"LORD POLONIUS At such a time I'll loose my daughter to him: Be you and I behind an arras then; Mark the encounter: if he love her not And be not from his reason fall'n thereon, Let me be no assistant for a state, But keep a farm and carters.\",\n",
       "       \"MARCELLUS Let's do't, I pray; and I this morning know Where we shall find him most conveniently.\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train['Sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[-,.?!;]\")\n",
    "def TagStringwithPunct(r):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(r)\n",
    "    tagged_list = pos_tag(tokenized)\n",
    "    \n",
    "    final_str = []\n",
    "    for word,tag in tagged_list:\n",
    "        final_str.append(tag+\"_\"+word)\n",
    "    \n",
    "    \n",
    "    return ' '.join(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving munged DATA into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train.to_pickle(folder +'/data_munged/pd_train_munged')\n",
    "pd_test.to_pickle(folder +'/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
