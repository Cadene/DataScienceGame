{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSG: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/youtube\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split,StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lib_DSG import ColumnSelector, DenseTransformer\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv('./data/train_sample_munged.csv', header=0, escapechar='\\\\', quotechar='\"', low_memory=False, error_bad_lines=False)\n",
    "pd_test = pd.read_csv('./data/test_sample_munged.csv', header=0, escapechar='\\\\', quotechar='\"', low_memory=False, error_bad_lines=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train = pd_train.fillna('')\n",
    "pd_test = pd_test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{3,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_3char': '(?u)\\\\b\\\\w+\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "\n",
    "stopwords = [u'http', u'com', u'www', u's', u'subscribe'\n",
    ", u'new', u'like', u'watch', u't', u'2014', u'1'\n",
    ", u'2013', u'2', u'la', u'en'\n",
    ", u'world', u'make', u'v', u'check', u'time'\n",
    ", u'10', u'best', u'3', u'5', u'day', u'y']\n",
    "tfv_title = TfidfVectorizer(lowercase=True, stop_words=stopwords, token_pattern=dico_pattern[\"match_word\"], \n",
    "                      ngram_range=(1, 1), max_df=0.5, min_df=2, max_features=50000, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "tfv_desc = TfidfVectorizer(lowercase=True, stop_words='english', token_pattern=dico_pattern[\"match_word\"], \n",
    "                      ngram_range=(1, 1), max_df=0.5, min_df=2, max_features=50000, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "tfv_topicid = TfidfVectorizer(lowercase=True, stop_words=None, token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "tfv_rel_topic = TfidfVectorizer(lowercase=True, stop_words=None, token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "clf = LogisticRegression( penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "                         intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear',\n",
    "                         max_iter=100, multi_class='ovr', verbose=0)\n",
    "\n",
    "title_pipe = make_pipeline(ColumnSelector(key='title'), tfv_title)\n",
    "desc_pipe = make_pipeline(ColumnSelector(key='description'), tfv_desc)\n",
    "topicId_pipe = make_pipeline(ColumnSelector(key=u'topicIds'), tfv_topicid)\n",
    "reltopicID_pipe = make_pipeline(ColumnSelector(key=u'relevantTopicIds'), tfv_rel_topic)\n",
    "\n",
    "pipeline = make_union(title_pipe, desc_pipe)\n",
    "pipeline.transformer_weights=[1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239225, 100000)\n"
     ]
    }
   ],
   "source": [
    "Y = pd_train[u'video_category_id'].values\n",
    "X = pipeline.fit_transform(pd_train) ; print X.shape\n",
    "X_test = pipeline.transform(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.01, 0.1, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.01, 0.2, 0.03)\n",
    "alphas = [1e-3, 1e-2, 1e-1, 1, 5]\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSS: acc: 0.2320, std: 0.0007, alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for i in alphas:\n",
    "    clf.C = i\n",
    "\n",
    "    sss = StratifiedShuffleSplit(Y, 5, test_size=0.2, random_state=0)\n",
    "    scores_sss = cross_val_score(clf, X, Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "\n",
    "    print (\"SSS: acc: %0.4f, std: %0.4f, alpha: %s\" %(scores_sss.mean(), scores_sss.std(), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = pd_train.apply(lambda r: (r['title'] + r['description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = [u'http', u'com', u'www', u's', u'subscribe'\n",
    ", u'new', u'like', u'watch', u't', u'2014', u'1'\n",
    ", u'2013', u'2', u'la', u'en'\n",
    ", u'world', u'make', u'v', u'check', u'time'\n",
    ", u'10', u'best', u'3', u'5', u'day', u'y']\n",
    "\n",
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_3char': '(?u)\\\\b\\\\w+\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "tfv = TfidfVectorizer(lowercase=True, stop_words='english', token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=0.5, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "#svd = TruncatedSVD(n_components=200, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "\n",
    "pipe = make_pipeline(tfv, svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239225, 200)\n"
     ]
    }
   ],
   "source": [
    "Y = pd_train[u'video_category_id'].values\n",
    "X = pipe.fit_transform(sentence)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'your recreations' u'\\u0e2d\\u0e1e' u'\\u043e\\u043d\\u043e' u'mieser'\n",
      " u'hayes tattoo'\n",
      " u'\\u0646\\u0642\\u0648\\u0645 \\u0628\\u0639\\u0631\\u0636\\u0647\\u0627'\n",
      " u'\\u0441\\u043e\\u0431\\u0430\\u043a\\u0435'\n",
      " u'\\u044d\\u0434\\u0438\\u043b\\u044c\\u0431\\u0430\\u0439'\n",
      " u'\\u043f\\u0440\\u0438\\u043d\\u044f\\u0442\\u0438\\u0435' u'undocumented' u'ztv'\n",
      " u'west films' u'\\u043c\\u0438\\u043d\\u0443\\u0441 \\u043a\\u043c'\n",
      " u'\\u043e\\u0431 \\u043e\\u0434\\u043d\\u043e\\u043c' u'shirt jcanell'\n",
      " u'\\u0645\\u062a\\u0631\\u062c\\u0645'\n",
      " u'\\u0644\\u0627 \\u0645\\u0643\\u0631\\u0645\\u0629'\n",
      " u'\\u0432\\u0438\\u0434\\u0430\\u043c' u'\\u0434\\u0442\\u043f'\n",
      " u'\\u0434\\u043e\\u0447\\u043a\\u0438' u'minecraftwhat'\n",
      " u'\\u0645\\u0635\\u0637\\u0641\\u0649' u'\\u043e\\u043d\\u0438 \\u043d\\u0435'\n",
      " u'witch minecraft' u'top 5who' u'country it'\n",
      " u'\\u0644\\u0644\\u0642\\u0648\\u0627\\u062a' u'with japan' u'take down'\n",
      " u'krave cereal' u'\\u043c\\u043e\\u043d\\u0442\\u0430\\u0436'\n",
      " u'\\u043c\\u043d\\u043e\\u0439' u'\\u043a\\u043e\\u043b\\u0435\\u0439'\n",
      " u'\\u062d\\u0633\\u0627\\u0628\\u064a \\u0641\\u064a'\n",
      " u'\\u0645\\u0648\\u0636\\u0648\\u0639' u'vandi' u'furrocious where'\n",
      " u'\\u0438\\u043d\\u0443\\u043f\\u043e\\u0440\\u043e\\u0434\\u0430 \\u0441\\u043e\\u0431\\u0430\\u043a'\n",
      " u'onboard gopro' u'furze vs']\n"
     ]
    }
   ],
   "source": [
    "numFeat = 40\n",
    "\n",
    "Features = np.array(tfv.get_feature_names())\n",
    "sorted_indices = np.argsort(np.array(X.sum(0))[0])[::-1]\n",
    "rankFeatures = Features[sorted_indices][:numFeat]\n",
    "print rankFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.01, 0.1]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.01, 0.2, 0.03)\n",
    "alphas = [1e-3, 1e-2, 1e-1]\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSS: acc: 0.3908, std: 0.0012, alpha: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-614f34a96cff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mscores_sss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"SSS: acc: %0.4f, std: %0.4f, alpha: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_sss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_sss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m                                               fit_params)\n\u001b[1;32m-> 1361\u001b[1;33m                       for train, test in cv)\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    664\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m                             \u001b[1;31m# We can now allow subprocesses again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m                             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__JOBLIB_SPAWNED_PARALLEL__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m                         \u001b[1;31m# Capture exception to add information on the local\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for i in alphas:\n",
    "    clf.C = i\n",
    "    sss = StratifiedShuffleSplit(Y, 5, test_size=0.2, random_state=0)\n",
    "    scores_sss = cross_val_score(clf, X, Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "    print (\"SSS: acc: %0.4f, std: %0.4f, alpha: %s\" %(scores_sss.mean(), scores_sss.std(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SUBMIT KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = pipeline.transform(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(index=None)\n",
    "submit['id']=pd_test['id']\n",
    "submit['Pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv(folder+'/results/0.arda1.csv',sep=';',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# FINAL DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pipe = make_pipeline(pipeline, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pipe.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
