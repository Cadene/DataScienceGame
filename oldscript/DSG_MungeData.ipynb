{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming raw DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/DSG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing raw data\n",
    "pd_train = pd.DataFrame.from_csv(folder + '/data/train_sample.csv',sep=';',header=0,index_col=None)\n",
    "pd_test = pd.DataFrame.from_csv(folder + '/data/test_sample.csv',sep=';',header=0,index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding some new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RemovePonctuationAndStemming(r):\n",
    "    new_r = tokenizer.tokenize(r)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    out = [stemmer.stem(word) for word in new_r]    \n",
    "    return out\n",
    "\n",
    "\n",
    "def addStuff(pd):\n",
    "    pd['lenght'] = pd['Sentence'].apply(lambda r: len(r)).astype(float)\n",
    "    \n",
    "    #number of CAPS character\n",
    "    pd['nb_caps_char'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter.isupper()])).astype(float)\n",
    "    \n",
    "    #number of CAPS word\n",
    "    pd['nb_caps_word'] = pd['Sentence'].apply(lambda r: np.sum([1 for word in r.split(\" \") if len(word) > 2 if word.isupper()])).astype(float)\n",
    "    \n",
    "    #one column for each punctuation token\n",
    "    pd['nb_?'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='?'])  )\n",
    "    \n",
    "    pd['nb_,'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==','])  )\n",
    "    \n",
    "    pd['nb_!'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='!'])  )\n",
    "    \n",
    "    pd['nb_.'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='.'])  )\n",
    "    \n",
    "    pd['nb_;'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==';'])  )\n",
    "    \n",
    "    pd[\"nb_'\"] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==\"'\"])  )\n",
    "    \n",
    "    pd['nb_all'] = pd['nb_?']+pd['nb_,']+pd['nb_!']+pd['nb_.']+pd['nb_;']+pd[\"nb_'\"]\n",
    "    \n",
    "    pd['nb_,;'] = pd['nb_,'] * pd['nb_;']\n",
    "    \n",
    "    #diversiy\n",
    "    pd['diversity'] = pd['Sentence'].apply(lambda r: float(len(np.unique(r.split(\" \")))) / float(len(r)))    \n",
    "    \n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train = addStuff(pd_train)\n",
    "pd_test = addStuff(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's onkores, Bilgewater?\",\n",
       "       \"He wouldn't ever dared to talk such talk in his life before.\",\n",
       "       \"Then she got to talking about her husband, and about her relations up the river, and her relations down the river, and about how much better off they used to was, and how they didn't know but they'd made a mistake coming to our town, instead of letting well aloneand so on and so on, till I was afeard I had made a mistake coming to her to find out what was going on in the town; but by and by she dropped on to pap and the murder, and then I was pretty willing to let her clatter right along.\",\n",
       "       ..., 'HAMLET Marry, this is miching mallecho; it means mischief.',\n",
       "       \"LORD POLONIUS At such a time I'll loose my daughter to him: Be you and I behind an arras then; Mark the encounter: if he love her not And be not from his reason fall'n thereon, Let me be no assistant for a state, But keep a farm and carters.\",\n",
       "       \"MARCELLUS Let's do't, I pray; and I this morning know Where we shall find him most conveniently.\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train['Sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[-,.?!;]\")\n",
    "def TagStringwithPunct(r):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(r)\n",
    "    tagged_list = pos_tag(tokenized)\n",
    "    \n",
    "    final_str = []\n",
    "    for word,tag in tagged_list:\n",
    "        final_str.append(tag+\"_\"+word)\n",
    "    \n",
    "    \n",
    "    return ' '.join(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving munged DATA into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train.to_pickle(folder +'/data_munged/pd_train_munged')\n",
    "pd_test.to_pickle(folder +'/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
