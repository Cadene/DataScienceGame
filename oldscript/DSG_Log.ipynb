{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model based on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/DSG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#feature extraction modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split,StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from lib_DSG import ColumnSelector, DenseTransformer\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_train = pd.io.pickle.read_pickle(folder + '/data_munged/pd_train_tagged')\n",
    "#pd_test = pd.io.pickle.read_pickle(folder + '/data_munged/pd_test_tagged')\n",
    "\n",
    "pd_train = pd.io.pickle.read_pickle(folder + '/data_munged/pd_train_munged')\n",
    "pd_test = pd.io.pickle.read_pickle(folder + '/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "tfv = TfidfVectorizer(lowercase=False, stop_words=None, token_pattern=dico_pattern[\"match_word_punct\"], \n",
    "                      ngram_range=(1, 2), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                   class_weight=None, random_state=None, solver='liblinear', max_iter=100,\n",
    "                   multi_class='ovr', verbose=0)\n",
    "\n",
    "pipeline = make_pipeline(ColumnSelector(key='Sentence'), tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd_train['Author'].values\n",
    "X = pipeline.fit_transform(pd_train) ; print X.shape\n",
    "X_test = pipeline.transform(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.3  0.5  0.7  0.9  1.1  1.3]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.1, 1.4, 0.2)\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF: acc: 0.5972, std: 0.0061, SSS: acc: 0.5803, std: 0.0042, alpha: 0.1\n",
      "SLF: acc: 0.7121, std: 0.0058, SSS: acc: 0.7035, std: 0.0052, alpha: 0.3\n",
      "SLF: acc: 0.7421, std: 0.0035, SSS: acc: 0.7367, std: 0.0060, alpha: 0.5\n",
      "SLF: acc: 0.7583, std: 0.0082, SSS: acc: 0.7533, std: 0.0065, alpha: 0.7\n",
      "SLF: acc: 0.7677, std: 0.0090, SSS: acc: 0.7639, std: 0.0048, alpha: 0.9\n",
      "SLF: acc: 0.7746, std: 0.0072, SSS: acc: 0.7706, std: 0.0051, alpha: 1.1\n",
      "SLF: acc: 0.7786, std: 0.0072, SSS: acc: 0.7763, std: 0.0049, alpha: 1.3\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for i in alphas:\n",
    "    clf.C = i\n",
    "    \n",
    "    skf = StratifiedKFold(Y, n_folds=10, indices=None, shuffle=True, random_state=None)\n",
    "    scores_skf = cross_val_score(clf, X, Y,scoring='accuracy',cv=skf, n_jobs=-1)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(Y, 10, test_size=0.2, random_state=0)\n",
    "    scores_sss = cross_val_score(clf, X, Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "\n",
    "    print (\"SLF: acc: %0.4f, std: %0.4f, SSS: acc: %0.4f, std: %0.4f, alpha: %s\" %\n",
    "           (scores_skf.mean(), scores_skf.std(), scores_sss.mean(), scores_sss.std(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8070 (+/- 0.0070)\n",
      "Accuracy: 0.8062 (+/- 0.0099)\n",
      "Accuracy: 0.8111 (+/- 0.0102)\n",
      "Accuracy: 0.8074 (+/- 0.0107)\n",
      "Accuracy: 0.8051 (+/- 0.0103)\n",
      "['austen' 'doyle' 'poe' 'shakespeare' 'twain' 'wilde']\n",
      "[[8558  367   31   57  215  559]\n",
      " [ 459 4121   80   62  375  508]\n",
      " [  13   31   89    5    9   13]\n",
      " [  85   60   14 1937  136   71]\n",
      " [ 220  412   17   87 5024  312]\n",
      " [ 611  497   12   44  236 3398]]\n"
     ]
    }
   ],
   "source": [
    "clf.C = 0.7\n",
    "\n",
    "for i in range(5):\n",
    "    scores=[]\n",
    "    confusion_mat = np.zeros([6,6],dtype=int)\n",
    "    for cv in range(5):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "        clf.fit(x_train,y_train)\n",
    "\n",
    "        scores.append(clf.score(x_val,y_val))\n",
    "        confusion_mat+=confusion_matrix(clf.predict(x_val),y_val)\n",
    "    print (\"Accuracy: %0.4f (+/- %0.4f)\" % (np.mean(scores), np.std(scores) * 2))\n",
    "        \n",
    "print clf.classes_\n",
    "print confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "\n",
    "tfv_uni = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_lowercase_only\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_bi = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_word1\"],\n",
    "                ngram_range=(2, 2), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_nnp = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_NNP\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_punctuation = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_punct\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                   class_weight=None, random_state=None, solver='liblinear', max_iter=100,\n",
    "                   multi_class='ovr', verbose=0)\n",
    "\n",
    "uni = make_pipeline(ColumnSelector(key='Sentence'), tfv_uni)\n",
    "bi = make_pipeline(ColumnSelector(key='Sentence'), tfv_bi)\n",
    "nnp = make_pipeline(ColumnSelector(key='Sentence'), tfv_nnp)\n",
    "punctuation = make_pipeline(ColumnSelector(key='Sentence'), tfv_punctuation)\n",
    "\n",
    "\n",
    "pipeline = make_union(uni, bi, nnp, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28723, 66630) 11438 53360 1825 7\n"
     ]
    }
   ],
   "source": [
    "Y = pd_train['Author'].values\n",
    "X = pipeline.fit_transform(pd_train)\n",
    "X_test = pipeline.transform(pd_test)\n",
    "\n",
    "print X.shape, len(tfv_uni.vocabulary_), len(tfv_bi.vocabulary_), len(tfv_nnp.vocabulary_), len(tfv_punctuation.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.1, 3, 0.5);\n",
    "alphas = [1., 2.]\n",
    "weights=[1]\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "SLF: acc: 0.7941, std: 0.0050, SSS: acc: 0.7899, std: 0.0048, alpha: 1.0\n",
      "SLF: acc: 0.8034, std: 0.0053, SSS: acc: 0.7981, std: 0.0049, alpha: 2.0\n"
     ]
    }
   ],
   "source": [
    "for weight in weights:\n",
    "    print weight\n",
    "    \n",
    "    pipeline.transformer_weights=[1.2, 1, 1., 1]\n",
    "    \n",
    "    tfv_uni.binary=True\n",
    "    tfv_bi.binary=True\n",
    "    tfv_nnp.binary=True\n",
    "    tfv_punctuation.binary=True\n",
    "    \n",
    "    X = pipeline.fit_transform(pd_train)\n",
    "    results=[]\n",
    "    for i in alphas:\n",
    "        clf.C = i\n",
    "        #clf.class_weight='auto'\n",
    "        \n",
    "        skf = StratifiedKFold(Y, n_folds=10, indices=None, shuffle=True, random_state=None)\n",
    "        scores_skf = cross_val_score(clf, X, Y,scoring='accuracy',cv=skf, n_jobs=-1)\n",
    "        \n",
    "        sss = StratifiedShuffleSplit(Y, 10, test_size=0.2, random_state=0)\n",
    "        scores_sss = cross_val_score(clf, X, Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "\n",
    "        print (\"SLF: acc: %0.4f, std: %0.4f, SSS: acc: %0.4f, std: %0.4f, alpha: %s\" %\n",
    "               (scores_skf.mean(), scores_skf.std(), scores_sss.mean(), scores_sss.std(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "#SLF: acc: 0.8114, std: 0.0073, SSS: acc: 0.8070, std: 0.0038, alpha: 1.2\n",
    "#SLF: acc: 0.8114, std: 0.0040, SSS: acc: 0.8073, std: 0.0033, alpha: 0.8\n",
    "#SLF: acc: 0.8120, std: 0.0078, SSS: acc: 0.8073, std: 0.0037, alpha: 0.7\n",
    "#SLF: acc: 0.8082, std: 0.0062, SSS: acc: 0.8079, std: 0.0037, alpha: 0.9 ===> 87,54527%\n",
    "\n",
    "\n",
    "#LOG\n",
    "#SLF: acc: 0.8012, std: 0.0044, SSS: acc: 0.8020, std: 0.0013, alpha: 60 ===> 86,8367%\n",
    "\n",
    "#best\n",
    "#SVM Accuracy: 0.7701 (+/- 0.0734), alpha: 1.3  ===> 88,57660% 'a','you','he','him','to','in','on' |auto|(1,2)|\n",
    "#SVM Accuracy: 0.7708 (+/- 0.0737), alpha: 1.9  ===> 88,43489%\n",
    "#SVM Accuracy: 0.7717 (+/- 0.0737), alpha: 2.4  ===> 88,39553\n",
    "#SVM Accuracy: 0.7709 (+/- 0.0764), alpha: 2.4  ===> 88,35616%\n",
    "#SVM Accuracy: 0.7663 (+/- 0.0764), alpha: 0.8  ===> 88,32467 %\n",
    "\n",
    "#best log Accuracy: 0.7683 (+/- 0.0798), alpha: 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7995 (+/- 0.0054), alpha: 0\n",
      "Accuracy: 0.7961 (+/- 0.0066), alpha: 1\n",
      "Accuracy: 0.7958 (+/- 0.0057), alpha: 2\n",
      "Accuracy: 0.7959 (+/- 0.0108), alpha: 3\n",
      "Accuracy: 0.7964 (+/- 0.0093), alpha: 4\n",
      "['austen' 'doyle' 'poe' 'shakespeare' 'twain' 'wilde']\n",
      "[[8968  589   43  145  368  752]\n",
      " [ 377 3991  130  109  453  461]\n",
      " [   0    0   18    0    0    0]\n",
      " [  25   16    5 1753   60   18]\n",
      " [ 185  423   35  131 5029  315]\n",
      " [ 458  416   20   61  252 3119]]\n",
      "Accuracy: 0.7968 (+/- 0.0093)\n"
     ]
    }
   ],
   "source": [
    "clf.C = 2.0\n",
    "l = []\n",
    "for i in range(5):\n",
    "\n",
    "    scores=[]\n",
    "    confusion_mat = np.zeros([6,6],dtype=int)\n",
    "    for cv in range(5):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size=0.2)\n",
    "        \n",
    "        clf.fit(x_train,y_train)\n",
    "        \n",
    "        scores.append(clf.score(x_val,y_val))\n",
    "        confusion_mat+=confusion_matrix(clf.predict(x_val),y_val)\n",
    "    l.append(np.mean(scores))\n",
    "    print (\"Accuracy: %0.4f (+/- %0.4f), alpha: %s\" % (np.mean(scores), np.std(scores) * 2, i))\n",
    "    \n",
    "print clf.classes_\n",
    "print confusion_mat\n",
    "print (\"Accuracy: %0.4f (+/- %0.4f)\" % (np.mean(l), np.std(scores) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Kaggle submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.7, class_weight='auto', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty=u'l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(index=None)\n",
    "submit['Id']=pd_test['Id']\n",
    "submit['Pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit.to_csv(folder+'/results/0.arda1.csv',sep=';',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wilde', 'doyle', 'austen', ..., 'doyle', 'twain', 'wilde'], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
