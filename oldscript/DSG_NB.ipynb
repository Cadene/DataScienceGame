{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSG: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/youtube/oldscript\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split,StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lib_DSG import ColumnSelector, DenseTransformer\n",
    "\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /home/arda/Documents/youtube/oldscript/data/train_only_text_simple.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-344c21703dd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_precision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmangle_dupe_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             infer_datetime_format=False, skip_blank_lines=True)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    472\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3173)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5912)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File /home/arda/Documents/youtube/oldscript/data/train_only_text_simple.csv does not exist"
     ]
    }
   ],
   "source": [
    "pd_train = pd.read_csv(folder + '/data/train_only_text_simple.csv', sep=';', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)\n",
    "\n",
    "\n",
    "pd_test = pd.read_csv(folder + '/data/test_only_text_simple.csv', sep=';', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "tfv = TfidfVectorizer(lowercase=False, stop_words=None, token_pattern=dico_pattern[\"match_word_punct\"], \n",
    "                      ngram_range=(1, 2), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.05, fit_prior=True, class_prior=None)\n",
    "\n",
    "pipeline = make_pipeline(ColumnSelector(key='Sentence'), tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28723, 70877)\n"
     ]
    }
   ],
   "source": [
    "Y = pd_train['Author'].values\n",
    "X = pipeline.fit_transform(pd_train) ; print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01  0.04  0.07  0.1   0.13  0.16  0.19]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.01, 0.2, 0.03)\n",
    "#alphas = [0.06]\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF: acc: 0.8193, std: 0.0057, SSS: acc: 0.8155, std: 0.0067, alpha: 0.01\n",
      "SLF: acc: 0.8245, std: 0.0064, SSS: acc: 0.8197, std: 0.0049, alpha: 0.04\n",
      "SLF: acc: 0.8231, std: 0.0078, SSS: acc: 0.8184, std: 0.0042, alpha: 0.07\n",
      "SLF: acc: 0.8214, std: 0.0059, SSS: acc: 0.8165, std: 0.0040, alpha: 0.1\n",
      "SLF: acc: 0.8190, std: 0.0054, SSS: acc: 0.8132, std: 0.0047, alpha: 0.13\n",
      "SLF: acc: 0.8132, std: 0.0063, SSS: acc: 0.8090, std: 0.0049, alpha: 0.16\n",
      "SLF: acc: 0.8084, std: 0.0077, SSS: acc: 0.8040, std: 0.0053, alpha: 0.19\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for i in alphas:\n",
    "    clf.alpha = i\n",
    "    \n",
    "    skf = StratifiedKFold(Y, n_folds=10, indices=None, shuffle=True, random_state=None)\n",
    "    scores_skf = cross_val_score(clf, X, Y,scoring='accuracy',cv=skf, n_jobs=-1)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(Y, 10, test_size=0.2, random_state=0)\n",
    "    scores_sss = cross_val_score(clf, X, Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "\n",
    "    print (\"SLF: acc: %0.4f, std: %0.4f, SSS: acc: %0.4f, std: %0.4f, alpha: %s\" %\n",
    "           (scores_skf.mean(), scores_skf.std(), scores_sss.mean(), scores_sss.std(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807371349096\n",
      "0.798538622129\n",
      "0.823181343543\n",
      "0.801253045597\n",
      "0.808844011142\n",
      "0.813022284123\n",
      "0.804665738162\n",
      "0.815395332637\n",
      "0.807665505226\n",
      "0.812195121951\n"
     ]
    }
   ],
   "source": [
    "clf.C=0.7\n",
    "\n",
    "skf = StratifiedKFold(Y, n_folds=10, indices=None, shuffle=True, random_state=None)\n",
    "for train_idx, val_idx in skf:\n",
    "    x_train, y_train, x_val, y_val = X[train_idx], Y[train_idx], X[val_idx], Y[val_idx]\n",
    "    \n",
    "    #x_train = kbest.fit_transform(x_train, y_train)\n",
    "    #x_val = kbest.transform(x_val)\n",
    "    \n",
    "    print clf.fit(x_train, y_train).score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "\n",
    "tfv_uni = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_lowercase_only\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_bi = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_word1\"],\n",
    "                ngram_range=(2, 2), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_nnp = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_NNP\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_punctuation = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_punct\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "uni = make_pipeline(ColumnSelector(key='Sentence'), tfv_uni)\n",
    "bi = make_pipeline(ColumnSelector(key='Sentence'), tfv_bi)\n",
    "nnp = make_pipeline(ColumnSelector(key='Sentence'), tfv_nnp)\n",
    "punctuation = make_pipeline(ColumnSelector(key='Sentence'), tfv_punctuation)\n",
    "\n",
    "\n",
    "pipeline = make_union(uni, bi, nnp, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (28723, 77821)\n"
     ]
    }
   ],
   "source": [
    "Y = pd_train['Author'].values\n",
    "X = pipeline.fit_transform(pd_train)\n",
    "X_test = pipeline.transform(pd_test)\n",
    "\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01  0.04  0.07  0.1   0.13  0.16  0.19]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0.01, 0.2, 0.03);\n",
    "weights=[1]\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "SLF: acc: 0.8202, std: 0.0084, SSS: acc: 0.8181, std: 0.0058, alpha: 0.01\n",
      "SLF: acc: 0.8267, std: 0.0064, SSS: acc: 0.8246, std: 0.0054, alpha: 0.04\n",
      "SLF: acc: 0.8277, std: 0.0064, SSS: acc: 0.8242, std: 0.0053, alpha: 0.07\n",
      "SLF: acc: 0.8251, std: 0.0116, SSS: acc: 0.8231, std: 0.0048, alpha: 0.1\n",
      "SLF: acc: 0.8263, std: 0.0063, SSS: acc: 0.8216, std: 0.0043, alpha: 0.13\n",
      "SLF: acc: 0.8244, std: 0.0041, SSS: acc: 0.8196, std: 0.0047, alpha: 0.16\n",
      "SLF: acc: 0.8233, std: 0.0051, SSS: acc: 0.8178, std: 0.0047, alpha: 0.19\n"
     ]
    }
   ],
   "source": [
    "for weight in weights:\n",
    "    print weight\n",
    "    \n",
    "    pipeline.transformer_weights=[1.3, 1, 1.1, 1]\n",
    "    \n",
    "    tfv_uni.binary=True\n",
    "    tfv_bi.binary=True\n",
    "    tfv_nnp.binary=True\n",
    "    tfv_punctuation.binary=True\n",
    "    \n",
    "    X = pipeline.fit_transform(pd_train)\n",
    "    results=[]\n",
    "    for i in alphas:\n",
    "        clf.alpha = i\n",
    "        \n",
    "        skf = StratifiedKFold(Y, n_folds=10, indices=None, shuffle=True, random_state=None)\n",
    "        scores_skf = cross_val_score(clf, X, Y,scoring='accuracy',cv=skf, n_jobs=-1)\n",
    "        \n",
    "        sss = StratifiedShuffleSplit(Y, 10, test_size=0.2, random_state=0)\n",
    "        scores_sss = cross_val_score(clf, X, Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "\n",
    "        print (\"SLF: acc: %0.4f, std: %0.4f, SSS: acc: %0.4f, std: %0.4f, alpha: %s\" %\n",
    "               (scores_skf.mean(), scores_skf.std(), scores_sss.mean(), scores_sss.std(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy: 0.7806 (+/- 0.0721), alpha: 0.08 ===>88,28531%\n",
    "#Accuracy: 0.7834 (+/- 0.0747), alpha: 0.05 ===>86,87608%\n",
    "#Accuracy: 0.7810 (+/- 0.0739), alpha: 0.04 ===>87,17525%\n",
    "#Accuracy: 0.7811 (+/- 0.0688), alpha: 0.14 ===>88,02551%.\n",
    "\n",
    "#SLF: acc: 0.8278, std: 0.0064, SSS: acc: 0.8249, std: 0.0062, alpha: 0.05 ===> 88,23020%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8251 (+/- 0.0115), alpha: 0\n",
      "Accuracy: 0.8236 (+/- 0.0065), alpha: 1\n",
      "Accuracy: 0.8247 (+/- 0.0084), alpha: 2\n",
      "Accuracy: 0.8199 (+/- 0.0060), alpha: 3\n",
      "Accuracy: 0.8283 (+/- 0.0054), alpha: 4\n",
      "Accuracy: 0.8188 (+/- 0.0051), alpha: 5\n",
      "Accuracy: 0.8233 (+/- 0.0066), alpha: 6\n",
      "Accuracy: 0.8256 (+/- 0.0028), alpha: 7\n",
      "Accuracy: 0.8253 (+/- 0.0100), alpha: 8\n",
      "Accuracy: 0.8240 (+/- 0.0071), alpha: 9\n",
      "['austen' 'doyle' 'poe' 'shakespeare' 'twain' 'wilde']\n",
      "[[8939  517   47   38  219  638]\n",
      " [ 246 4219   93   49  320  439]\n",
      " [   0    5   50    0    2    1]\n",
      " [  62   57   21 1906  208   52]\n",
      " [ 124  374   18   63 5159  255]\n",
      " [ 447  482   17   37  226 3395]]\n",
      "Accuracy: 0.8239 (+/- 0.0071)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.07\n",
    "\n",
    "l = []\n",
    "for i in range(10):\n",
    "    \n",
    "    NB= MultinomialNB(alpha=alpha , fit_prior=True, class_prior=None)\n",
    "\n",
    "    scores=[]\n",
    "    confusion_mat = np.zeros([6,6],dtype=int)\n",
    "    for cv in range(5):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size=0.2)\n",
    "        \n",
    "        NB.fit(x_train,y_train)\n",
    "        \n",
    "        scores.append(NB.score(x_val,y_val))\n",
    "        confusion_mat+=confusion_matrix(NB.predict(x_val),y_val)\n",
    "    l.append(np.mean(scores))\n",
    "    print (\"Accuracy: %0.4f (+/- %0.4f), alpha: %s\" % (np.mean(scores), np.std(scores) * 2, i))\n",
    "    \n",
    "print NB.classes_\n",
    "print confusion_mat\n",
    "print (\"Accuracy: %0.4f (+/- %0.4f)\" % (np.mean(l), np.std(scores) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SUBMIT KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.04, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(index=None)\n",
    "submit['Id']=pd_test['Id']\n",
    "submit['Pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv(folder+'/results/0.arda1.csv',sep=';',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
