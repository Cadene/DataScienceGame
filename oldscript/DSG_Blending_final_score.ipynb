{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Blending of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best score so far\n",
    "#alpha = 1.6 beta = 1. predict_proba, NB + log 88,94662%\n",
    "#alpha = .3 beta = .2 predict_proba, NB + log 88,95450\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check scores with train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/DSG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LinearRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split,StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from lib_DSG import TransformerMixin, ColumnSelector\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_train = pd.io.pickle.read_pickle(folder + '/data_munged/pd_train_tagged')\n",
    "#pd_test = pd.io.pickle.read_pickle(folder + '/data_munged/pd_test_tagged')\n",
    "\n",
    "pd_train = pd.io.pickle.read_pickle(folder + '/data_munged/pd_train_munged')\n",
    "pd_test = pd.io.pickle.read_pickle(folder + '/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "\n",
    "tfv_uni = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_lowercase_only\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_bi = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_word1\"],\n",
    "                ngram_range=(2, 2), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_nnp = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_NNP\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_punctuation = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_punct\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "log = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=2.0, fit_intercept=True, intercept_scaling=1, \n",
    "                   class_weight=None, random_state=None, solver='liblinear', max_iter=100,\n",
    "                   multi_class='ovr', verbose=0)\n",
    "\n",
    "uni = make_pipeline(ColumnSelector(key='Sentence'), tfv_uni)\n",
    "bi = make_pipeline(ColumnSelector(key='Sentence'), tfv_bi)\n",
    "nnp = make_pipeline(ColumnSelector(key='Sentence'), tfv_nnp)\n",
    "punctuation = make_pipeline(ColumnSelector(key='Sentence'), tfv_punctuation)\n",
    "\n",
    "\n",
    "log_pipe = make_union(uni, bi, nnp, punctuation)\n",
    "log_pipe.transformer_weights=[1.2, 1, 1., 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "\n",
    "tfv_uni = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_lowercase_only\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_bi = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_word1\"],\n",
    "                ngram_range=(2, 2), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_nnp = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_NNP\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_punctuation = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_punct\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "NB = MultinomialNB(alpha=0.05, fit_prior=True, class_prior=None)\n",
    "\n",
    "uni = make_pipeline(ColumnSelector(key='Sentence'), tfv_uni)\n",
    "bi = make_pipeline(ColumnSelector(key='Sentence'), tfv_bi)\n",
    "nnp = make_pipeline(ColumnSelector(key='Sentence'), tfv_nnp)\n",
    "punctuation = make_pipeline(ColumnSelector(key='Sentence'), tfv_punctuation)\n",
    "\n",
    "\n",
    "NB_pipe = make_union(uni, bi, nnp, punctuation)\n",
    "NB_pipe.transformer_weights=[1.3, 1, 1.1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "\n",
    "tfv_uni = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_lowercase_only\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_bi = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_word1\"],\n",
    "                ngram_range=(2, 2), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_nnp = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_NNP\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfv_punctuation = TfidfVectorizer(lowercase=False, analyzer=u'word', stop_words=None, token_pattern=dico_pattern[\"match_punct\"],\n",
    "                ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, vocabulary=None,\n",
    "                binary=True, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr',\n",
    "          fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0,\n",
    "          random_state=None, max_iter=1000)\n",
    "\n",
    "uni = make_pipeline(ColumnSelector(key='Sentence'), tfv_uni)\n",
    "bi = make_pipeline(ColumnSelector(key='Sentence'), tfv_bi)\n",
    "nnp = make_pipeline(ColumnSelector(key='Sentence'), tfv_nnp)\n",
    "punctuation = make_pipeline(ColumnSelector(key='Sentence'), tfv_punctuation)\n",
    "\n",
    "\n",
    "svm_pipe = make_union(uni, bi, nnp, punctuation)\n",
    "svm_pipe.transformer_weights=[1.2, 1, 1.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd_train['Author'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF: acc: 0.8017, std: 0.0058, SSS: acc: 0.8014, std: 0.0024\n",
      "SLF: acc: 0.8006, std: 0.0020, SSS: acc: 0.8014, std: 0.0024\n",
      "SLF: acc: 0.8016, std: 0.0041, SSS: acc: 0.8014, std: 0.0024\n"
     ]
    }
   ],
   "source": [
    "#X = sgd_pipe.fit_transform(pd_train)\n",
    "#X = log_pipe.fit_transform(pd_train)\n",
    "#X = NB_pipe.fit_transform(pd_train)\n",
    "X = svm_pipe.fit_transform(pd_train)\n",
    "#X = rf_pipe.fit_transform(pd_train)\n",
    "#X = ridge_pipe.fit_transform(pd_train)\n",
    "\n",
    "for cv in range(3):\n",
    "    \n",
    "    clf=svm\n",
    "    \n",
    "    skf = StratifiedKFold(Y, n_folds=5, indices=None, shuffle=True, random_state=None)\n",
    "    scores_skf = cross_val_score(clf, X,Y,scoring='accuracy',cv=skf, n_jobs=-1)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(Y, 5, test_size=0.2, random_state=0)\n",
    "    scores_sss = cross_val_score(clf, X,Y,scoring='accuracy',cv=sss, n_jobs=-1)\n",
    "\n",
    "    print (\"SLF: acc: %0.4f, std: %0.4f, SSS: acc: %0.4f, std: %0.4f\" %\n",
    "           (scores_skf.mean(), scores_skf.std(), scores_sss.mean(), scores_sss.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLENDING : meta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=14, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(pd_train,Y, test_size=0.2, random_state=345)\n",
    "\n",
    "X_nb  = NB_pipe.fit_transform(x_train)\n",
    "X_log = log_pipe.fit_transform(x_train)\n",
    "#X_tag = tag_pipe.fit_transform(x_train)\n",
    "#X_svm = svm_pipe.fit_transform(x_train)\n",
    "X_rf = rf_pipe.fit_transform(x_train)\n",
    "\n",
    "\n",
    "\n",
    "Xval_nb  = NB_pipe.transform(x_val)\n",
    "Xval_log = log_pipe.transform(x_val)\n",
    "#Xval_tag = tag_pipe.transform(x_val)\n",
    "#Xval_svm = svm_pipe.transform(x_val)\n",
    "Xval_rf = rf_pipe.transform(x_val)\n",
    "\n",
    "#fitting on original data\n",
    "NB.fit(X_nb, y_train)\n",
    "log.fit(X_log, y_train)\n",
    "#svm.fit(X_svm, y_train)\n",
    "rf.fit(X_rf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22978, 95735)\n",
      "(22978, 58411)\n",
      "(22978, 57779)\n"
     ]
    }
   ],
   "source": [
    "print X_svm.shape\n",
    "print X_log.shape\n",
    "print X_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metaXtrain = np.hstack((NB.predict_proba(X_nb),log.predict_proba(X_log), rf.predict_proba(X_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0008, 0.001, 0.003, 0.006, 0.009, 0.012]\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 1., 10., 100.] \n",
    "\n",
    "alphas = 10.**-np.arange(1,7)\n",
    "#alphas = [9e-3, 1e-4, 2e-4, 3e-4, 6e-4, 15e-4]\n",
    "#alphas = [0.0006]\n",
    "#alphas = np.arange(1, 5, 1)\n",
    "alphas = [0.8e-3, 1e-3, 3e-3, 6e-3, 9e-3, 12e-3]\n",
    "\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807832898172\n",
      "0.803307223673\n",
      "0.812880765883\n",
      "0.815491731941\n",
      "\n",
      "\n",
      "0.804351610096\n",
      "0.797389033943\n",
      "0.81044386423\n",
      "0.815317667537\n",
      "\n",
      "\n",
      "0.798955613577\n",
      "0.798955613577\n",
      "0.805221932115\n",
      "0.811314186249\n",
      "\n",
      "\n",
      "0.802610966057\n",
      "0.801914708442\n",
      "0.805221932115\n",
      "0.818450826806\n",
      "\n",
      "\n",
      "0.812184508268\n",
      "0.8\n",
      "0.80591818973\n",
      "0.821235857267\n",
      "\n",
      "\n",
      "0.816187989556\n",
      "0.805395996519\n",
      "0.80818102698\n",
      "0.826631853786\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 8.5\n",
    "beta = 8.0\n",
    "\n",
    "\n",
    "for i in alphas:\n",
    "    meta_log = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=i, fit_intercept=True, \n",
    "                                  intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear',\n",
    "                                  max_iter=100, multi_class='ovr', verbose=0)\n",
    "    \n",
    "    meta_svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=i, multi_class='ovr',\n",
    "                       fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0,\n",
    "                       random_state=None, max_iter=1000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    clf = meta_svm\n",
    "    \n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(pd_train,Y, test_size=0.2, random_state=np.random.randint(1000))\n",
    "\n",
    "    X_nb  = NB_pipe.fit_transform(x_train)\n",
    "    X_log = log_pipe.fit_transform(x_train)\n",
    "    #X_tag = tag_pipe.fit_transform(x_train)\n",
    "    X_svm = svm_pipe.fit_transform(x_train)\n",
    "    #X_rf = rf_pipe.fit_transform(x_train)\n",
    "\n",
    "    Xval_nb  = NB_pipe.transform(x_val)\n",
    "    Xval_log = log_pipe.transform(x_val)\n",
    "    #Xval_tag = tag_pipe.transform(x_val)\n",
    "    Xval_svm = svm_pipe.transform(x_val)\n",
    "    #Xval_rf = rf_pipe.transform(x_val)\n",
    "\n",
    "    #fitting on original data\n",
    "    NB.fit(X_nb, y_train)\n",
    "    log.fit(X_log, y_train)\n",
    "    svm.fit(X_svm, y_train)\n",
    "    #rf.fit(X_rf, y_train)\n",
    "\n",
    "    #building metaX\n",
    "    metaXtrain = np.hstack((NB.predict_proba(X_nb),log.predict_proba(X_log)))\n",
    "    #metaXtrain = np.hstack((NB.predict_proba(X_nb), log.predict_proba(X_log), rf.predict_proba(X_rf) ))\n",
    "    metaY = y_train\n",
    "\n",
    "    #fitting META clf\n",
    "\n",
    "    clf.fit(metaXtrain, metaY)\n",
    "    #meta_svm.fit(metaXtrain, metaY)\n",
    "\n",
    "    #building eval data\n",
    "    nb_pred = NB.predict_proba(Xval_nb)\n",
    "    log_pred = log.predict_proba(Xval_log)\n",
    "    #rf_pred = rf.predict_proba(Xval_rf)\n",
    "\n",
    "    #metaXval = np.hstack((nb_pred,log_pred,rf_pred))\n",
    "    metaXval = np.hstack((nb_pred,log_pred))\n",
    "    \n",
    "    y_pred_log = clf.predict(metaXval)\n",
    "    #y_pred_svm = meta_svm.predict(metaXval)\n",
    "    pred_normal_mean = clf.classes_[(( alpha*nb_pred + beta*log_pred) /2.).argmax(axis=1)]\n",
    "    \n",
    "    \n",
    "    print accuracy_score(NB.predict(Xval_nb), y_val)\n",
    "    print accuracy_score(log.predict(Xval_log), y_val)\n",
    "    \n",
    "    print accuracy_score(y_pred_log, y_val)\n",
    "    #print accuracy_score(y_pred_svm, y_val)\n",
    "    \n",
    "    print accuracy_score(pred_normal_mean, y_val)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805744125326\n",
      "0.799303742385\n",
      "0.756657963446\n",
      "\n",
      "\n",
      "0.794255874674\n",
      "0.79181897302\n",
      "0.746910356832\n",
      "\n",
      "\n",
      "0.807832898172\n",
      "0.795648389904\n",
      "0.755613577023\n",
      "\n",
      "\n",
      "0.79773716275\n",
      "0.791993037424\n",
      "0.749869451697\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-1e5780f51c97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mX_nb\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mNB_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mX_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#X_tag = tag_pipe.fit_transform(x_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \"\"\"\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mn_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_sort_features\u001b[1;34m(self, X, vocabulary)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mmap_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 8.5\n",
    "beta = 8.0\n",
    "\n",
    "\n",
    "for i in alphas:\n",
    "    meta_log = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=i, fit_intercept=True, \n",
    "                                  intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear',\n",
    "                                  max_iter=100, multi_class='ovr', verbose=0)\n",
    "    \n",
    "    meta_svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=i, multi_class='ovr',\n",
    "                       fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0,\n",
    "                       random_state=None, max_iter=1000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    clf = meta_svm\n",
    "    \n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(pd_train,Y, test_size=0.2, random_state=np.random.randint(1000))\n",
    "\n",
    "    X_nb  = NB_pipe.fit_transform(x_train)\n",
    "    X_log = log_pipe.fit_transform(x_train)\n",
    "    #X_tag = tag_pipe.fit_transform(x_train)\n",
    "    X_svm = svm_pipe.fit_transform(x_train)\n",
    "    #X_rf = rf_pipe.fit_transform(x_train)\n",
    "\n",
    "    Xval_nb  = NB_pipe.transform(x_val)\n",
    "    Xval_log = log_pipe.transform(x_val)\n",
    "    #Xval_tag = tag_pipe.transform(x_val)\n",
    "    Xval_svm = svm_pipe.transform(x_val)\n",
    "    #Xval_rf = rf_pipe.transform(x_val)\n",
    "\n",
    "    #fitting on original data\n",
    "    NB.fit(X_nb, y_train)\n",
    "    log.fit(X_log, y_train)\n",
    "    svm.fit(X_svm, y_train)\n",
    "    #rf.fit(X_rf, y_train)\n",
    "\n",
    "    #building metaX\n",
    "    dic = {'austen': 1, 'doyle': 2, 'poe': 3, 'shakespeare': 4, 'twain': 5, 'wilde': 6}\n",
    "    dic1 = {1:'austen', 2:'doyle', 3:'poe', 4:'shakespeare', 5:'twain', 6:'wilde'}\n",
    "    metaXtrain = pd.DataFrame(np.vstack((NB.predict(X_nb), log.predict(X_log), svm.predict(X_svm))).T).apply(lambda r: [dic[x] for x in r.values])\n",
    "    metaY = y_train\n",
    "    \n",
    "    #fitting META clf\n",
    "    clf.fit(metaXtrain, metaY)\n",
    "    #meta_svm.fit(metaXtrain, metaY)\n",
    "    \n",
    "    #building eval data\n",
    "    nb_pred = NB.predict(Xval_nb)\n",
    "    log_pred = log.predict(Xval_log)\n",
    "    #rf_pred = rf.predict_proba(Xval_rf)\n",
    "    svm_pred = svm.predict(Xval_svm)\n",
    "\n",
    "    #metaXval = np.hstack((nb_pred,log_pred,rf_pred))\n",
    "    metaXval = pd.DataFrame(np.vstack((nb_pred, log_pred, svm_pred)).T).apply(lambda r: [dic[x] for x in r.values])\n",
    "    \n",
    "    y_pred = clf.predict(metaXval)\n",
    "    #y_pred_svm = meta_svm.predict(metaXval)    \n",
    "    y_pred = np.round(metaXval.mean(axis=1)).apply(lambda r: dic1[r])\n",
    "    \n",
    "    \n",
    "    print accuracy_score(NB.predict(Xval_nb), y_val)\n",
    "    print accuracy_score(log.predict(Xval_log), y_val)\n",
    "    \n",
    "    print accuracy_score(y_pred, y_val)\n",
    "    \n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BLENDING : Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getProbaSVM(dist_mat):\n",
    "\n",
    "    dist_mat = dist_mat - dist_mat.min()\n",
    "\n",
    "    dist_mat = dist_mat / dist_mat.max()\n",
    "\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### proba prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(pd_train,Y, test_size=0.2, random_state=np.random.randint(1000))\n",
    "\n",
    "X_nb  = NB_pipe.fit_transform(x_train)\n",
    "X_log = log_pipe.fit_transform(x_train)\n",
    "#X_tag = tag_pipe.fit_transform(x_train)\n",
    "#X_tag = tag_pipe.fit_transform(x_train)\n",
    "X_svm = svm_pipe.fit_transform(x_train)\n",
    "\n",
    "Xval_nb  = NB_pipe.transform(x_val)\n",
    "Xval_log = log_pipe.transform(x_val)\n",
    "#Xval_tag = tag_pipe.transform(x_val)\n",
    "Xval_svm = svm_pipe.transform(x_val)\n",
    "\n",
    "nb_pred = NB.fit(X_nb,y_train).predict_proba(Xval_nb)\n",
    "log_pred = log.fit(X_log,y_train).predict_proba(Xval_log)\n",
    "#tag_pred = tag.fit(X_tag,y_train).predict_proba(Xval_tag)\n",
    "mat_dist = svm.fit(X_svm, y_train).decision_function(Xval_svm)\n",
    "svm_pred = getProbaSVM(mat_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model 3 params: alpha, beta, gamma | 3models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "score NB: 0.810617928634\n",
      "score log: 0.790948651001\n",
      "score svm: 0.793385552654\n",
      "params normal mean [ 0.2       0.1       0.8       0.822106  0.820191]\n",
      "params geo mean [ 0.4       0.1       1.1       0.820191  0.821236]\n",
      "\n",
      "\n",
      "score NB: 0.807832898172\n",
      "score log: 0.793211488251\n",
      "score svm: 0.78955613577\n",
      "params normal mean [ 0.3       0.1       1.3       0.817058  0.815318]\n",
      "params geo mean [ 0.2       0.1       0.6       0.816188  0.816188]\n",
      "\n",
      "\n",
      "score NB: 0.818276762402\n",
      "score log: 0.798607484769\n",
      "score svm: 0.798781549173\n",
      "params normal mean [ 0.6       0.2       1.1       0.823673  0.821584]\n",
      "params geo mean [ 0.9       0.2       1.3       0.822628  0.823325]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-116f13785e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mscore_normal_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_normal_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mscore_geo_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_geo_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_normal_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_geo_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiffering_labels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_weighted_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cv in range(5):\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(pd_train,Y, test_size=0.2)\n",
    "\n",
    "    X_nb  = NB_pipe.fit_transform(x_train)\n",
    "    X_log = log_pipe.fit_transform(x_train)\n",
    "    #X_tag = tag_pipe.fit_transform(x_train)\n",
    "    #X_rf = rf_pipe.fit_transform(x_train)\n",
    "    X_svm = svm_pipe.fit_transform(x_train)\n",
    "\n",
    "    Xval_nb  = NB_pipe.transform(x_val)\n",
    "    Xval_log = log_pipe.transform(x_val)\n",
    "    #Xval_tag = tag_pipe.transform(x_val)\n",
    "    #Xval_rf = rf_pipe.transform(x_val)\n",
    "    Xval_svm = svm_pipe.transform(x_val)\n",
    "\n",
    "\n",
    "    #nb_pred = NB.fit(X_nb,y_train).predict_proba(Xval_nb)\n",
    "    #log_pred = log.fit(X_log,y_train).predict_proba(Xval_log)\n",
    "    #tag_pred = tag.fit(X_tag,y_train).predict_proba(Xval_tag)\n",
    "    #rf_pred = rf.fit(X_rf,y_train).predict_proba(Xval_rf)\n",
    "    #mat_dist = svm.fit(X_svm, y_train).decision_function(Xval_svm)\n",
    "    #svm_pred = getProbaSVM(mat_dist)    \n",
    "    \n",
    "    nb_pred = np.log(1 + NB.fit(X_nb,y_train).predict_proba(Xval_nb))\n",
    "    log_pred = np.log(1 +log.fit(X_log,y_train).predict_proba(Xval_log))\n",
    "    mat_dist = svm.fit(X_svm, y_train).decision_function(Xval_svm)\n",
    "    svm_pred = np.log(1+getProbaSVM(mat_dist))\n",
    "    \n",
    "    results = []\n",
    "    classes = NB.classes_\n",
    "\n",
    "    for alpha in np.arange(0,1.5,.1):\n",
    "        for beta in np.arange(0,1.5,.1):\n",
    "            for gamma in np.arange(0,1.5,.1):\n",
    "\n",
    "                pred_normal_mean = classes[( (alpha*nb_pred + beta*log_pred + gamma*svm_pred) ).argmax(axis=1)]\n",
    "                pred_geo_mean = classes[(  nb_pred**(alpha) * log_pred**(beta) * svm_pred**(gamma)  ).argmax(axis=1)]\n",
    "\n",
    "                #pred_normal_mean = classes[ (alpha*nb_pred + beta*log_pred).argmax(axis=1) ]\n",
    "                #pred_geo_mean = classes[(  nb_pred**(alpha) * log_pred**(beta) ).argmax(axis=1)]\n",
    "                \n",
    "                score_normal_mean = accuracy_score(pred_normal_mean,y_val)\n",
    "                score_geo_mean = accuracy_score(pred_geo_mean,y_val)\n",
    "\n",
    "                results.append((alpha, beta, gamma, score_normal_mean, score_geo_mean))\n",
    "                \n",
    "    results = np.around(np.array(results),decimals=6)\n",
    "    print \"\\n\"\n",
    "    print \"score NB:\", accuracy_score(classes[nb_pred.argmax(axis=1)],y_val)\n",
    "    print \"score log:\", accuracy_score(classes[log_pred.argmax(axis=1)],y_val)\n",
    "    print \"score svm:\", accuracy_score(classes[svm_pred.argmax(axis=1)],y_val)\n",
    "\n",
    "    print \"params normal mean\", results[(results[:,3]).argmax()]\n",
    "    print \"params geo mean\", results[(results[:,4]).argmax()]\n",
    "    #print \"params harmonic mean\", results[(np.asarray(results)[:,4]).argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score NB: 0.810269799826\n",
      "score svm: 0.795648389904\n",
      "score normal mean 0.821758050479\n",
      "score geo mean 0.820713664056\n",
      "diff : 0.0114882506527\n",
      "diff : 0.0104438642298\n",
      "score NB: 0.813402959095\n",
      "score svm: 0.801392515231\n",
      "score normal mean 0.824891209748\n",
      "score geo mean 0.824194952132\n",
      "diff : 0.0114882506527\n",
      "diff : 0.0107919930374\n",
      "score NB: 0.813925152306\n",
      "score svm: 0.791993037424\n",
      "score normal mean 0.819321148825\n",
      "score geo mean 0.819669277633\n",
      "diff : 0.00539599651871\n",
      "diff : 0.00574412532637\n",
      "score NB: 0.817406440383\n",
      "score svm: 0.792863359443\n",
      "score normal mean 0.82454308094\n",
      "score geo mean 0.822454308094\n",
      "diff : 0.00713664055701\n",
      "diff : 0.00504786771105\n"
     ]
    }
   ],
   "source": [
    "alpha = .2\n",
    "beta = .1\n",
    "gamma = 1.\n",
    "\n",
    "for cv in range(4):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(pd_train,Y, test_size=0.2)    \n",
    "    \n",
    "    X_nb  = NB_pipe.fit_transform(x_train)\n",
    "    X_log = log_pipe.fit_transform(x_train)\n",
    "    #X_tag = tag_pipe.fit_transform(x_train)\n",
    "    #X_rf = rf_pipe.fit_transform(x_train)\n",
    "    X_svm = svm_pipe.fit_transform(x_train)\n",
    "\n",
    "    Xval_nb  = NB_pipe.transform(x_val)\n",
    "    Xval_log = log_pipe.transform(x_val)\n",
    "    #Xval_tag = tag_pipe.transform(x_val)\n",
    "    #Xval_rf = rf_pipe.transform(x_val)\n",
    "    Xval_svm = svm_pipe.transform(x_val)\n",
    "\n",
    "    #nb_pred = NB.fit(X_nb,y_train).predict_proba(Xval_nb)\n",
    "    #log_pred = log.fit(X_log,y_train).predict_proba(Xval_log)\n",
    "    #tag_pred = tag.fit(X_tag,y_train).predict_proba(Xval_tag)\n",
    "    #rf_pred = rf.fit(X_rf,y_train).predict_proba(Xval_rf)\n",
    "    #mat_dist = svm.fit(X_svm, y_train).decision_function(Xval_svm)\n",
    "    #svm_pred = getProbaSVM(mat_dist)    \n",
    "    \n",
    "    nb_pred = np.log(1 + NB.fit(X_nb,y_train).predict_proba(Xval_nb))\n",
    "    log_pred = np.log(1 +log.fit(X_log,y_train).predict_proba(Xval_log))\n",
    "    mat_dist = svm.fit(X_svm, y_train).decision_function(Xval_svm)\n",
    "    svm_pred = np.log(1 +getProbaSVM(mat_dist))\n",
    "    \n",
    "    \n",
    "    pred_normal_mean = classes[( (alpha*nb_pred + beta*log_pred + gamma*svm_pred) ).argmax(axis=1)]\n",
    "    pred_geo_mean = classes[(  nb_pred**(alpha) * log_pred**(beta) * svm_pred**(gamma)  ).argmax(axis=1)]\n",
    "\n",
    "    #pred_normal_mean = classes[ (alpha*nb_pred + beta*log_pred).argmax(axis=1) ]\n",
    "    #pred_geo_mean = classes[(  nb_pred**(alpha) * log_pred**(beta) ).argmax(axis=1)]\n",
    "    \n",
    "    score_normal_mean = accuracy_score(pred_normal_mean,y_val)\n",
    "    score_geo_mean = accuracy_score(pred_geo_mean,y_val)\n",
    "\n",
    "    \n",
    "    print \"score NB:\", accuracy_score(classes[nb_pred.argmax(axis=1)],y_val)\n",
    "    print \"score svm:\", accuracy_score(classes[svm_pred.argmax(axis=1)],y_val)\n",
    "    \n",
    "    print \"score normal mean\", score_normal_mean\n",
    "    print \"score geo mean\", score_geo_mean\n",
    "    #print \"score harm mean\", score_harm_mean\n",
    "    print \"diff :\" , (score_normal_mean-accuracy_score(classes[nb_pred.argmax(axis=1)],y_val))\n",
    "    print \"diff :\" , (score_geo_mean-accuracy_score(classes[nb_pred.argmax(axis=1)],y_val))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kaggle final fitting on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = .2\n",
    "beta = .1\n",
    "gamma = 1.\n",
    "\n",
    "Y = pd_train['Author']\n",
    "\n",
    "X_nb  = NB_pipe.fit_transform(pd_train)\n",
    "X_log = log_pipe.fit_transform(pd_train)\n",
    "#X_tag = tag_pipe.fit_transform(x_train)\n",
    "#X_rf = rf_pipe.fit_transform(x_train)\n",
    "X_svm = svm_pipe.fit_transform(pd_train)\n",
    "\n",
    "Xval_nb  = NB_pipe.transform(pd_test)\n",
    "Xval_log = log_pipe.transform(pd_test)\n",
    "#Xval_tag = tag_pipe.transform(x_val)\n",
    "#Xval_rf = rf_pipe.transform(x_val)\n",
    "Xval_svm = svm_pipe.transform(pd_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nb_pred = np.log(1 + NB.fit(X_nb,Y).predict_proba(Xval_nb))\n",
    "log_pred = np.log(1 +log.fit(X_log,Y).predict_proba(Xval_log))\n",
    "mat_dist = svm.fit(X_svm, Y).decision_function(Xval_svm)\n",
    "svm_pred = np.log(1 +getProbaSVM(mat_dist))\n",
    "\n",
    "\n",
    "pred_normal_mean = classes[( (alpha*nb_pred + beta*log_pred + gamma*svm_pred) ).argmax(axis=1)]\n",
    "#pred_normal_mean = classes[ (alpha*nb_pred + beta*svm_pred).argmax(axis=1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = pred_normal_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prediction majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(index=None)\n",
    "submit['Id']=pd_test['Id']\n",
    "submit['Pred']=np.asarray(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wilde', 'doyle', 'austen', ..., 'doyle', 'twain', 'wilde'], \n",
       "      dtype='|S11')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit.to_csv(folder+'/results/0.arda1.csv',sep=';',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
