{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming raw DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/youtube\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv(folder + '/data/train_sample.csv', sep=',', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)\n",
    "\n",
    "\n",
    "pd_test = pd.read_csv(folder + '/data/test_sample.csv', sep=',', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'video_category_id', u'title', u'description', u'published_at',\n",
      "       u'viewCount', u'likeCount', u'dislikeCount', u'favoriteCount',\n",
      "       u'commentCount', u'duration', u'dimension', u'definition', u'caption',\n",
      "       u'licensedContent', u'topicIds', u'relevantTopicIds'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print pd_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split topicIds\n",
    "pd_train[u'topicIds'] = pd_train[u'topicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "pd_test[u'topicIds'] = pd_test[u'topicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "\n",
    "#split\n",
    "pd_train[u'relevantTopicIds'] = pd_train[u'relevantTopicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "pd_test[u'relevantTopicIds'] = pd_test[u'relevantTopicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_category_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>dislikeCount</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>dimension</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>licensedContent</th>\n",
       "      <th>topicIds</th>\n",
       "      <th>relevantTopicIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>19</td>\n",
       "      <td>臺灣觀光六大主題「讚！臺灣」生態篇(3分鐘版)；\"Bravo! Taiwan\"-NATURE...</td>\n",
       "      <td>2014-05-20T09:34:11.000Z</td>\n",
       "      <td>27559</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>PT3M1S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>False</td>\n",
       "      <td>/m/0zwm09x</td>\n",
       "      <td>[/m/06f32, /m/011s0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_category_id                                              title  \\\n",
       "16646                 19  臺灣觀光六大主題「讚！臺灣」生態篇(3分鐘版)；\"Bravo! Taiwan\"-NATURE...   \n",
       "\n",
       "                    description published_at  viewCount  likeCount  \\\n",
       "16646  2014-05-20T09:34:11.000Z        27559        356          1   \n",
       "\n",
       "       dislikeCount  favoriteCount commentCount duration dimension definition  \\\n",
       "16646             0             12       PT3M1S       2d        hd      false   \n",
       "\n",
       "      caption licensedContent              topicIds relevantTopicIds  \n",
       "16646   False      /m/0zwm09x  [/m/06f32, /m/011s0]               []  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train[pd_train['published_at'].apply(lambda r: len(r)!=24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "5         False\n",
       "6         False\n",
       "7         False\n",
       "8         False\n",
       "9         False\n",
       "10        False\n",
       "11        False\n",
       "12        False\n",
       "13        False\n",
       "14        False\n",
       "15        False\n",
       "16        False\n",
       "17        False\n",
       "18        False\n",
       "19        False\n",
       "20        False\n",
       "21        False\n",
       "22        False\n",
       "23        False\n",
       "24        False\n",
       "25        False\n",
       "26        False\n",
       "27        False\n",
       "28        False\n",
       "29        False\n",
       "          ...  \n",
       "239195    False\n",
       "239196    False\n",
       "239197    False\n",
       "239198    False\n",
       "239199    False\n",
       "239200    False\n",
       "239201    False\n",
       "239202    False\n",
       "239203    False\n",
       "239204    False\n",
       "239205    False\n",
       "239206    False\n",
       "239207    False\n",
       "239208    False\n",
       "239209    False\n",
       "239210    False\n",
       "239211    False\n",
       "239212    False\n",
       "239213    False\n",
       "239214    False\n",
       "239215    False\n",
       "239216    False\n",
       "239217    False\n",
       "239218    False\n",
       "239219    False\n",
       "239220    False\n",
       "239221    False\n",
       "239222    False\n",
       "239223    False\n",
       "239224    False\n",
       "Name: published_at, dtype: bool"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "pd_train['published_at'].apply(lambda r: len(r)!=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "year is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-bbdd91dead9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2058\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2060\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2061\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:58435)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-bbdd91dead9f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(r)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mrepl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweekday\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: year is out of range"
     ]
    }
   ],
   "source": [
    "pd_train['published_at'].apply(lambda r: parser.parse(r) if )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-11-15T17:00:06.000Z\n"
     ]
    }
   ],
   "source": [
    "test = pd_train['published_at'][0] ; print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2014\n"
     ]
    }
   ],
   "source": [
    "print d.strftime('%m/%d/%Y') #==> '09/26/2008'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding some new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RemovePonctuationAndStemming(r):\n",
    "    new_r = tokenizer.tokenize(r)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    out = [stemmer.stem(word) for word in new_r]    \n",
    "    return out\n",
    "\n",
    "\n",
    "def addStuff(pd):\n",
    "    pd['lenght'] = pd['Sentence'].apply(lambda r: len(r)).astype(float)\n",
    "    \n",
    "    #number of CAPS character\n",
    "    pd['nb_caps_char'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter.isupper()])).astype(float)\n",
    "    \n",
    "    #number of CAPS word\n",
    "    pd['nb_caps_word'] = pd['Sentence'].apply(lambda r: np.sum([1 for word in r.split(\" \") if len(word) > 2 if word.isupper()])).astype(float)\n",
    "    \n",
    "    #one column for each punctuation token\n",
    "    pd['nb_?'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='?'])  )\n",
    "    \n",
    "    pd['nb_,'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==','])  )\n",
    "    \n",
    "    pd['nb_!'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='!'])  )\n",
    "    \n",
    "    pd['nb_.'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='.'])  )\n",
    "    \n",
    "    pd['nb_;'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==';'])  )\n",
    "    \n",
    "    pd[\"nb_'\"] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==\"'\"])  )\n",
    "    \n",
    "    pd['nb_all'] = pd['nb_?']+pd['nb_,']+pd['nb_!']+pd['nb_.']+pd['nb_;']+pd[\"nb_'\"]\n",
    "    \n",
    "    pd['nb_,;'] = pd['nb_,'] * pd['nb_;']\n",
    "    \n",
    "    #diversiy\n",
    "    pd['diversity'] = pd['Sentence'].apply(lambda r: float(len(np.unique(r.split(\" \")))) / float(len(r)))    \n",
    "    \n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train = addStuff(pd_train)\n",
    "pd_test = addStuff(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's onkores, Bilgewater?\",\n",
       "       \"He wouldn't ever dared to talk such talk in his life before.\",\n",
       "       \"Then she got to talking about her husband, and about her relations up the river, and her relations down the river, and about how much better off they used to was, and how they didn't know but they'd made a mistake coming to our town, instead of letting well aloneand so on and so on, till I was afeard I had made a mistake coming to her to find out what was going on in the town; but by and by she dropped on to pap and the murder, and then I was pretty willing to let her clatter right along.\",\n",
       "       ..., 'HAMLET Marry, this is miching mallecho; it means mischief.',\n",
       "       \"LORD POLONIUS At such a time I'll loose my daughter to him: Be you and I behind an arras then; Mark the encounter: if he love her not And be not from his reason fall'n thereon, Let me be no assistant for a state, But keep a farm and carters.\",\n",
       "       \"MARCELLUS Let's do't, I pray; and I this morning know Where we shall find him most conveniently.\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train['Sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[-,.?!;]\")\n",
    "def TagStringwithPunct(r):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(r)\n",
    "    tagged_list = pos_tag(tokenized)\n",
    "    \n",
    "    final_str = []\n",
    "    for word,tag in tagged_list:\n",
    "        final_str.append(tag+\"_\"+word)\n",
    "    \n",
    "    \n",
    "    return ' '.join(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving munged DATA into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train.to_pickle(folder +'/data_munged/pd_train_munged')\n",
    "pd_test.to_pickle(folder +'/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
