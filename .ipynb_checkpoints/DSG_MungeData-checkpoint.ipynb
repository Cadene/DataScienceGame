{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming raw DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/Documents/youtube\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "\n",
    "folder = os.getcwd() ; print folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv(folder + '/data/train_sample.csv', sep=',', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)\n",
    "\n",
    "\n",
    "pd_test = pd.read_csv(folder + '/data/test_sample.csv', sep=',', dialect=None, compression='infer', doublequote=True, \n",
    "            escapechar='\\\\', quotechar='\"', quoting=0, skipinitialspace=False,\n",
    "            lineterminator=None, header='infer', index_col=None, names=None,\n",
    "            prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None,\n",
    "            na_fvalues=None, true_values=None, false_values=None, delimiter=None, converters=None,\n",
    "            dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False,\n",
    "            na_filter=True, compact_ints=False, use_unsigned=False, low_memory=False, buffer_lines=None,\n",
    "            warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None,\n",
    "            decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None,\n",
    "            memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,\n",
    "            verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False,\n",
    "            infer_datetime_format=False, skip_blank_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239225, 16)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'video_category_id', u'title', u'description', u'published_at',\n",
      "       u'viewCount', u'likeCount', u'dislikeCount', u'favoriteCount',\n",
      "       u'commentCount', u'duration', u'dimension', u'definition', u'caption',\n",
      "       u'licensedContent', u'topicIds', u'relevantTopicIds'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print pd_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split topicIds\n",
    "pd_train[u'topicIds'] = pd_train[u'topicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "pd_test[u'topicIds'] = pd_test[u'topicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "\n",
    "#split\n",
    "pd_train[u'relevantTopicIds'] = pd_train[u'relevantTopicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])\n",
    "pd_test[u'relevantTopicIds'] = pd_test[u'relevantTopicIds'].apply(lambda r: r.split(\";\") if type(r)==str else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pd_train[pd_train['published_at'].apply(lambda r: len(r)!=24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2014-11-15 17:00:06\n",
       "1        2013-09-27 10:56:23\n",
       "2        2014-05-27 21:57:18\n",
       "3        2013-07-30 23:00:18\n",
       "4        2014-11-09 19:49:21\n",
       "5        2014-03-20 08:02:45\n",
       "6        2014-10-09 17:48:12\n",
       "7        2013-07-03 15:00:18\n",
       "8        2013-08-19 23:15:19\n",
       "9        2014-06-24 18:10:03\n",
       "10       2014-01-25 16:29:21\n",
       "11       2013-09-22 05:17:21\n",
       "12       2014-07-11 11:49:12\n",
       "13       2013-11-13 15:24:26\n",
       "14       2014-12-17 19:46:12\n",
       "15       2014-01-21 01:20:44\n",
       "16       2014-03-27 16:00:04\n",
       "17       2013-12-12 09:28:34\n",
       "18       2014-09-25 04:00:36\n",
       "19       2014-04-07 10:17:49\n",
       "20       2013-01-04 18:14:47\n",
       "21       2013-01-15 21:50:58\n",
       "22       2014-10-02 09:01:23\n",
       "23       2014-04-15 20:15:34\n",
       "24       2014-10-02 20:53:34\n",
       "25       2014-12-06 09:43:36\n",
       "26       2014-04-01 11:00:01\n",
       "27       2014-01-01 16:00:19\n",
       "28       2013-02-15 22:16:22\n",
       "29       2014-03-26 03:41:50\n",
       "                 ...        \n",
       "239195   2013-03-21 04:00:15\n",
       "239196   2013-01-13 19:41:32\n",
       "239197   2014-03-22 22:43:56\n",
       "239198   2014-09-06 16:02:08\n",
       "239199   2014-05-02 15:57:42\n",
       "239200   2014-12-29 14:00:08\n",
       "239201   2013-04-05 23:46:00\n",
       "239202   2013-10-03 23:00:32\n",
       "239203   2014-04-25 20:09:16\n",
       "239204   2013-04-12 17:00:14\n",
       "239205   2014-01-19 21:44:48\n",
       "239206   2013-05-08 22:11:04\n",
       "239207   2014-10-25 20:00:09\n",
       "239208   2013-08-26 16:54:58\n",
       "239209   2014-02-19 19:55:54\n",
       "239210   2014-08-08 00:33:21\n",
       "239211   2014-02-06 21:00:02\n",
       "239212   2013-06-06 23:27:26\n",
       "239213   2013-06-11 14:14:51\n",
       "239214   2014-10-12 17:00:06\n",
       "239215   2013-04-02 17:30:14\n",
       "239216   2014-12-02 18:49:43\n",
       "239217   2013-09-06 00:41:43\n",
       "239218   2013-01-20 18:00:58\n",
       "239219   2013-06-20 04:00:48\n",
       "239220   2013-08-14 07:39:57\n",
       "239221   2013-03-09 18:00:42\n",
       "239222   2014-07-07 23:41:12\n",
       "239223   2014-03-08 22:00:01\n",
       "239224   2014-01-20 20:30:46\n",
       "Name: published_at, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "pd_train['published_at'] =  pd_train['published_at'].apply(lambda r: parser.parse(r[:19]) if len(r)==24 else datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd_train['published_at'].apply(lambda r: time.strptime(r[:19], \"%Y-%m-%dT%H:%M:%S\") if len(r)==24 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd_train['published_at'].apply(lambda r: time.strptime(r[:19], \"%Y-%m-%dT%H:%M:%S\") if len(r)==24 else datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           (2014, 11, 15, 17, 0, 6, 5, 319, -1)\n",
       "1          (2013, 9, 27, 10, 56, 23, 4, 270, -1)\n",
       "2          (2014, 5, 27, 21, 57, 18, 1, 147, -1)\n",
       "3           (2013, 7, 30, 23, 0, 18, 1, 211, -1)\n",
       "4          (2014, 11, 9, 19, 49, 21, 6, 313, -1)\n",
       "5             (2014, 3, 20, 8, 2, 45, 3, 79, -1)\n",
       "6          (2014, 10, 9, 17, 48, 12, 3, 282, -1)\n",
       "7            (2013, 7, 3, 15, 0, 18, 2, 184, -1)\n",
       "8          (2013, 8, 19, 23, 15, 19, 0, 231, -1)\n",
       "9           (2014, 6, 24, 18, 10, 3, 1, 175, -1)\n",
       "10          (2014, 1, 25, 16, 29, 21, 5, 25, -1)\n",
       "11          (2013, 9, 22, 5, 17, 21, 6, 265, -1)\n",
       "12         (2014, 7, 11, 11, 49, 12, 4, 192, -1)\n",
       "13        (2013, 11, 13, 15, 24, 26, 2, 317, -1)\n",
       "14        (2014, 12, 17, 19, 46, 12, 2, 351, -1)\n",
       "15           (2014, 1, 21, 1, 20, 44, 1, 21, -1)\n",
       "16            (2014, 3, 27, 16, 0, 4, 3, 86, -1)\n",
       "17         (2013, 12, 12, 9, 28, 34, 3, 346, -1)\n",
       "18           (2014, 9, 25, 4, 0, 36, 3, 268, -1)\n",
       "19           (2014, 4, 7, 10, 17, 49, 0, 97, -1)\n",
       "20            (2013, 1, 4, 18, 14, 47, 4, 4, -1)\n",
       "21          (2013, 1, 15, 21, 50, 58, 1, 15, -1)\n",
       "22           (2014, 10, 2, 9, 1, 23, 3, 275, -1)\n",
       "23         (2014, 4, 15, 20, 15, 34, 1, 105, -1)\n",
       "24         (2014, 10, 2, 20, 53, 34, 3, 275, -1)\n",
       "25          (2014, 12, 6, 9, 43, 36, 5, 340, -1)\n",
       "26             (2014, 4, 1, 11, 0, 1, 1, 91, -1)\n",
       "27             (2014, 1, 1, 16, 0, 19, 2, 1, -1)\n",
       "28          (2013, 2, 15, 22, 16, 22, 4, 46, -1)\n",
       "29           (2014, 3, 26, 3, 41, 50, 2, 85, -1)\n",
       "                           ...                  \n",
       "239195        (2013, 3, 21, 4, 0, 15, 3, 80, -1)\n",
       "239196      (2013, 1, 13, 19, 41, 32, 6, 13, -1)\n",
       "239197      (2014, 3, 22, 22, 43, 56, 5, 81, -1)\n",
       "239198        (2014, 9, 6, 16, 2, 8, 5, 249, -1)\n",
       "239199      (2014, 5, 2, 15, 57, 42, 4, 122, -1)\n",
       "239200      (2014, 12, 29, 14, 0, 8, 0, 363, -1)\n",
       "239201        (2013, 4, 5, 23, 46, 0, 4, 95, -1)\n",
       "239202      (2013, 10, 3, 23, 0, 32, 3, 276, -1)\n",
       "239203      (2014, 4, 25, 20, 9, 16, 4, 115, -1)\n",
       "239204      (2013, 4, 12, 17, 0, 14, 4, 102, -1)\n",
       "239205      (2014, 1, 19, 21, 44, 48, 6, 19, -1)\n",
       "239206       (2013, 5, 8, 22, 11, 4, 2, 128, -1)\n",
       "239207      (2014, 10, 25, 20, 0, 9, 5, 298, -1)\n",
       "239208     (2013, 8, 26, 16, 54, 58, 0, 238, -1)\n",
       "239209      (2014, 2, 19, 19, 55, 54, 2, 50, -1)\n",
       "239210       (2014, 8, 8, 0, 33, 21, 4, 220, -1)\n",
       "239211         (2014, 2, 6, 21, 0, 2, 3, 37, -1)\n",
       "239212      (2013, 6, 6, 23, 27, 26, 3, 157, -1)\n",
       "239213     (2013, 6, 11, 14, 14, 51, 1, 162, -1)\n",
       "239214      (2014, 10, 12, 17, 0, 6, 6, 285, -1)\n",
       "239215       (2013, 4, 2, 17, 30, 14, 1, 92, -1)\n",
       "239216     (2014, 12, 2, 18, 49, 43, 1, 336, -1)\n",
       "239217       (2013, 9, 6, 0, 41, 43, 4, 249, -1)\n",
       "239218       (2013, 1, 20, 18, 0, 58, 6, 20, -1)\n",
       "239219       (2013, 6, 20, 4, 0, 48, 3, 171, -1)\n",
       "239220      (2013, 8, 14, 7, 39, 57, 2, 226, -1)\n",
       "239221        (2013, 3, 9, 18, 0, 42, 5, 68, -1)\n",
       "239222      (2014, 7, 7, 23, 41, 12, 0, 188, -1)\n",
       "239223         (2014, 3, 8, 22, 0, 1, 5, 67, -1)\n",
       "239224      (2014, 1, 20, 20, 30, 46, 0, 20, -1)\n",
       "Name: published_at, dtype: object"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-122-635663f67587>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-122-635663f67587>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    pd_train['published_at'].apply(lambda r: parser.parse(r) if len(r)!=24)\u001b[0m\n\u001b[1;37m                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "year is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-bbdd91dead9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2058\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2060\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2061\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:58435)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-bbdd91dead9f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(r)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'published_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/python_dateutil-2.4.2-py2.7.egg/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mrepl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweekday\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: year is out of range"
     ]
    }
   ],
   "source": [
    "pd_train['published_at'].apply(lambda r: parser.parse(r) if )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-11-15T17:00:06.000Z\n"
     ]
    }
   ],
   "source": [
    "test = pd_train['published_at'][0] ; print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2014\n"
     ]
    }
   ],
   "source": [
    "print d.strftime('%m/%d/%Y') #==> '09/26/2008'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding some new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RemovePonctuationAndStemming(r):\n",
    "    new_r = tokenizer.tokenize(r)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    out = [stemmer.stem(word) for word in new_r]    \n",
    "    return out\n",
    "\n",
    "\n",
    "def addStuff(pd):\n",
    "    pd['lenght'] = pd['Sentence'].apply(lambda r: len(r)).astype(float)\n",
    "    \n",
    "    #number of CAPS character\n",
    "    pd['nb_caps_char'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter.isupper()])).astype(float)\n",
    "    \n",
    "    #number of CAPS word\n",
    "    pd['nb_caps_word'] = pd['Sentence'].apply(lambda r: np.sum([1 for word in r.split(\" \") if len(word) > 2 if word.isupper()])).astype(float)\n",
    "    \n",
    "    #one column for each punctuation token\n",
    "    pd['nb_?'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='?'])  )\n",
    "    \n",
    "    pd['nb_,'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==','])  )\n",
    "    \n",
    "    pd['nb_!'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='!'])  )\n",
    "    \n",
    "    pd['nb_.'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter=='.'])  )\n",
    "    \n",
    "    pd['nb_;'] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==';'])  )\n",
    "    \n",
    "    pd[\"nb_'\"] = pd['Sentence'].apply(lambda r: np.sum([1 for letter in r if letter==\"'\"])  )\n",
    "    \n",
    "    pd['nb_all'] = pd['nb_?']+pd['nb_,']+pd['nb_!']+pd['nb_.']+pd['nb_;']+pd[\"nb_'\"]\n",
    "    \n",
    "    pd['nb_,;'] = pd['nb_,'] * pd['nb_;']\n",
    "    \n",
    "    #diversiy\n",
    "    pd['diversity'] = pd['Sentence'].apply(lambda r: float(len(np.unique(r.split(\" \")))) / float(len(r)))    \n",
    "    \n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train = addStuff(pd_train)\n",
    "pd_test = addStuff(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n",
    "\n",
    "pd_train['Sentence_stem_punct'] = pd_train['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )\n",
    "pd_test['Sentence_stem_punct'] = pd_test['Sentence_stem_punct'].apply(lambda r: ' '.join(RemovePonctuationAndStemming(r)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's onkores, Bilgewater?\",\n",
       "       \"He wouldn't ever dared to talk such talk in his life before.\",\n",
       "       \"Then she got to talking about her husband, and about her relations up the river, and her relations down the river, and about how much better off they used to was, and how they didn't know but they'd made a mistake coming to our town, instead of letting well aloneand so on and so on, till I was afeard I had made a mistake coming to her to find out what was going on in the town; but by and by she dropped on to pap and the murder, and then I was pretty willing to let her clatter right along.\",\n",
       "       ..., 'HAMLET Marry, this is miching mallecho; it means mischief.',\n",
       "       \"LORD POLONIUS At such a time I'll loose my daughter to him: Be you and I behind an arras then; Mark the encounter: if he love her not And be not from his reason fall'n thereon, Let me be no assistant for a state, But keep a farm and carters.\",\n",
       "       \"MARCELLUS Let's do't, I pray; and I this morning know Where we shall find him most conveniently.\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train['Sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence'].apply(lambda r: ''.join([char for char in r if ord(char)<128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[-,.?!;]\")\n",
    "def TagStringwithPunct(r):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(r)\n",
    "    tagged_list = pos_tag(tokenized)\n",
    "    \n",
    "    final_str = []\n",
    "    for word,tag in tagged_list:\n",
    "        final_str.append(tag+\"_\"+word)\n",
    "    \n",
    "    \n",
    "    return ' '.join(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train['Sentence_tag'] = pd_train['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_test['Sentence_tag'] = pd_test['Sentence_tag'].apply(lambda r: TagStringwithPunct(r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving munged DATA into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train.to_pickle(folder +'/data_munged/pd_train_munged')\n",
    "pd_test.to_pickle(folder +'/data_munged/pd_test_munged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
