{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from lib_DSG import ColumnSelector, DenseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv('./data/train_sample_munged.csv', header=0, escapechar='\\\\', quotechar='\"', low_memory=False)\n",
    "pd_test = pd.read_csv('./data/test_sample_munged.csv', header=0, escapechar='\\\\', quotechar='\"', low_memory=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train = pd_train.fillna('')\n",
    "pd_test = pd_test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dico_pattern={'match_lowercase_only':'\\\\b[a-z]+\\\\b',\n",
    "              'match_word':'\\\\w{1,}',\n",
    "              'match_word1': '(?u)\\\\b\\\\w+\\\\b',\n",
    "              'match_word_punct': '\\w+|[,.?!;]',\n",
    "              'match_NNP': '\\\\b[A-Z][a-z]+\\\\b|\\\\b[A-Z]+\\\\b',\n",
    "              'match_punct': \"[,.?!;'-]\"\n",
    "             }\n",
    "tfv_title = TfidfVectorizer(lowercase=True, stop_words='english', token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "tfv_desc = TfidfVectorizer(lowercase=True, stop_words='english', token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "tfv_topicid = TfidfVectorizer(lowercase=True, stop_words=None, token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "tfv_rel_topic = TfidfVectorizer(lowercase=True, stop_words=None, token_pattern=dico_pattern[\"match_word1\"], \n",
    "                      ngram_range=(1, 1), max_df=1.0, min_df=2, max_features=None, \n",
    "                      vocabulary=None, binary=True, norm=u'l2', \n",
    "                      use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.05, fit_prior=True, class_prior=None)\n",
    "\n",
    "title_pipe = make_pipeline(ColumnSelector(key='title'), tfv_title)\n",
    "desc_pipe = make_pipeline(ColumnSelector(key='description'), tfv_desc)\n",
    "topicId_pipe = make_pipeline(ColumnSelector(key=u'topicIds'), tfv_topicid)\n",
    "reltopicID_pipe = make_pipeline(ColumnSelector(key=u'relevantTopicIds'), tfv_rel_topic)\n",
    "\n",
    "pipeline = make_union(title_pipe, desc_pipe, topicId_pipe, reltopicID_pipe)\n",
    "pipeline.transformer_weights=[1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd_train[u'video_category_id'].values\n",
    "X = pipeline.fit_transform(pd_train) ; print X.shape\n",
    "X_test = pipeline.transform(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # seed to shuffle the train set\n",
    "\n",
    "n_folds = 10\n",
    "verbose = True\n",
    "shuffle = True\n",
    "\n",
    "X, y, X_submission = load_data.load()\n",
    "\n",
    "if shuffle:\n",
    "    idx = np.random.permutation(y.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "clfs = [MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None),\n",
    "        MultinomialNB(alpha=0.1, fit_prior=True, class_prior=None)]\n",
    "\n",
    "print \"Creating train and test sets for blending.\"\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print j, clf\n",
    "    dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Fold\", i\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:,1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]\n",
    "    dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "print\n",
    "print \"Blending.\"\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, y)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:,1]\n",
    "\n",
    "print \"Linear stretch of predictions to [0,1]\"\n",
    "y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "\n",
    "print \"Saving Results.\"\n",
    "np.savetxt(fname='test.csv', X=y_submission, fmt='%0.9f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(attempt, actual, epsilon=1.0e-15):\n",
    "    \"\"\"Logloss, i.e. the score of the bioresponse competition.\n",
    "    \"\"\"\n",
    "    attempt = np.clip(attempt, epsilon, 1.0-epsilon)\n",
    "    return - np.mean(actual * np.log(attempt) + (1.0 - actual) * np.log(1.0 - attempt))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(0) # seed to shuffle the train set\n",
    "\n",
    "    n_folds = 10\n",
    "    verbose = True\n",
    "    shuffle = False\n",
    "\n",
    "    X, y, X_submission = load_data.load()\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(learn_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    print \"Creating train and test sets for blending.\"\n",
    "    \n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print j, clf\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print \"Fold\", i\n",
    "            X_train = X[train]\n",
    "            y_train = y[train]\n",
    "            X_test = X[test]\n",
    "            y_test = y[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)[:,1]\n",
    "            dataset_blend_train[test, j] = y_submission\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]\n",
    "        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "    print\n",
    "    print \"Blending.\"\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(dataset_blend_train, y)\n",
    "    y_submission = clf.predict_proba(dataset_blend_test)[:,1]\n",
    "\n",
    "    print \"Linear stretch of predictions to [0,1]\"\n",
    "    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "\n",
    "    print \"Saving Results.\"\n",
    "    np.savetxt(fname='test.csv', X=y_submission, fmt='%0.9f')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
